## 附件六 可信賴AI檢核表(範本)
可信賴AI檢核表

可解釋性 (Explainability)
AI 發展與應用階段，應致力權衡決策生成之準確性與可解釋性，兼顧使用者及受影響者權益，故AI 技術所生成之決策，應盡力以文字、視覺、範例等關係人可理解之方式與內容，對於AI 系統、軟體、演算法等技術之使用者與受影響者，進行事後的說明、展現與解釋。
準確性	
    1	你是否可確保用於開發AI模型的資料（包括訓練、測試資料）是最新、高品質、完整且在預計部署系統的環境中具有代表性？	■是	說明：本研究對資料進行嚴格的測試與清理，並使用各式場址，以確保對台灣地區的農用情景具代表性。
			
			
	2	你是否已建立監控及記錄AI模型準確性之步驟？	■是	說明：在本研究中，我們使用常見農作物缺水容忍度作為基準，定期以後驗式方法評估與驗證模型的表現，並對訓練得到的模型進行特徵重要性分析，以確保AI模型的準確性和可解釋性。
			
			
	3	AI模型的準確性程度是否可能導致危急、不利或破壞性的後果？	
			■否	
			
	4	你可曾考慮過，AI模型的運作是否會使資料或原本訓練時所設定的假設無效？若會，可能導致何種不利影響？	■是	說明：本模型不會隨著終端使用者操作或持續運行而改變參數，故無造成原本訓練資料及相關假定失效之疑慮。
			
			
	5	你是否訂定程序，以確保能向終端使用者妥適溝通、傳達其所期待的AI模型準確性程度？	■是	說明：我們目前在使用者互動介面上已對本模型進行基本的說明，讓使用者了解模型本身與模型提供之產品(決策)之限制，並將持續完善這方面的資訊。


			
			
可解釋性	
    6	你是否能向使用者解釋AI模型的決策？	■是	說明：在研究團隊內部，我們定期使用特徵重要性分析工具分析訓練得到的模型，結果足以解釋AI模型給出的決策。
			
			
	7	你是否有機制能持續調查使用者對AI決策的了解程度？	
			■否	
			
透明性與可追溯性 (Transparency and Traceability)
AI 所生成之決策對於利害關係人有重大影響，為保障決策過程之公正性，在AI 系統、軟體及演算法等技術發展與應用上，包括但不限於對於模組、機制、組成、參數及計算等進行最低限度的資訊提供與揭露，以確保一般人得以知悉人工智慧系統生成決策之要素。
AI 技術之發展與應用須遵循可追溯性要求，對於決策過程中包括但不限於資料蒐集、資料標籤以及所使用的演算法進行適當記錄，並建立相關紀錄保存制度，以利受AI 技術決策影響之利害關係人得為事後救濟及釐清。
可追溯性	
    8	你是否可以追溯AI模型使用哪些資料、演算法或規則作出特定決策或建議？	■是	說明：
			
			
	9	你是否持續評估AI模型輸入資料之品質？	■是	說明：
			
			
	10	你是否持續評估AI模型輸出結果之品質？	■是	說明：
			
			
	11	你是否已透過日誌紀錄，將AI模型之決策或建議記錄下來？	
			■否	
			
透明性	
    12	AI模型是否被設計用於與終端使用者進行互動(interact)、引導或幫忙作決策？	
			■否	
			
	13	你是否已預先模擬AI模型與潛在使用者之間的社會互動？	
			
			■不適用	
	14	你是否建立機制讓使用者知悉其正與人類還是機器互動？	
			
			■不適用	
	15	使用者是否能意識到某項建議或結果是源於AI演算法決策？	■是	說明：
			
			
個人隱私與資料治理 (Privacy and Data Governance)
個人資料隱私侵害的預防，必須建立有效的數據治理，在AI 研發與應用上，AI 科研人員應致力注意個人資料蒐集、處理及利用符合相關法令規範，以保障人性尊嚴與基本人權，並針對AI 系統內部之個人資料架構有適當的管理措施，以維護資料當事人權益。
個人隱私	
    16	你是否藉由使用或處理個人資料（包括特種個人資料）對AI模型進行訓練或開發？	
			
			■不適用	
	17	你是否考慮到AI模型對隱私權以及個人資料保護的影響，並建立機制以識別隱私及個人資料保護問題？	
			
			■不適用	
資料治理	
    18	你是否建立隱私和個人資料風險評估及管理機制（例如：隱私衝擊評估；個人資料蒐集、處理及利用之內部管理程序；資料近用或修改之權限控管；使用紀錄、軌跡資料及證據保存）？	
			
			■不適用	
	19	是否採取措施達到隱私保護設計和預設(privacy by design/privacy by default)與資料最少化(data minimization)（例如：加密、假名化及匿名化）？	
			
			■不適用	
	20	在AI模型開發過程中，對於所蒐集的個人資料，你是否讓資料當事人有撤回同意、選擇退出(opt-out)或資料刪除的機會？	
			
			■不適用	
公平與非歧視性 (Fairness and Non-discrimination)
AI科研人員應致力於AI系統、軟體、演算法等技術及進行決策時，落實以人為本，平等尊重所有人之基本人權與人性尊嚴，避免產生偏差與歧視等風險，並建立外部回饋機制。
公平性	
    21	你是否已評估AI模型對終端使用者可能造成不公平的風險？ 	
			
			■不適用	
	22	除了終端使用者之外，你是否已識別出可能受到AI模型間接或直接影響的潛在主體？	
			
			■不適用	
	23	有關公平性的定義，你是否已徵詢利害關係群體（包括弱勢族群等）之意見？	
			
			■不適用	
	24	針對公平性，你是否有定量分析或量測指標？	
			
			■不適用	
非歧視性	
    25	你是否已識別AI模型相關偏誤、歧視或表現不良等問題，並建立一套機制，以避免產生或加劇AI模型中不公平的偏誤（包括輸入資料的使用及演算法的設計等）？	■是	說明：
			
			
	26	你是否已藉由公開可得或符合當代技術水準的技術工具，改善你的資料、模型與效能(performance)？	■是	說明：
			
			
	27	你是否已建立教育訓練計畫，幫助AI設計者和AI開發者在設計與開發AI模型過程中更加意識到可能的偏誤？	
			■否	
			
	28	當發現AI模型生命週期中的潛在偏誤時，你是否有明確的處理步驟和問題通報管道（包括如何通報、向誰通報）？	
			■否	
			
利害關係人之參與	
    29	你是否建立機制， 使最大可能範圍的利害關係人能參與AI模型設計與開發階段（例如：徵詢潛在使用者（包括弱勢族群）之意見）？	
			
			■不適用	
安全性 (safety)
AI 科研人員應致力於AI 系統、軟體、演算法等技術運作環境之安全性，包括但不限於穩健性、網路與資訊安全、風險控管與監測等，並追求AI 系統合理且善意的使用，構築安全可靠之AI 環境。
一般安全	
    30	針對AI模型的安全性，你是否建立風險評估機制（例如：風險定義、風險指標、風險等級、風險評估與持續量測流程等）？	
			
			■不適用	
	31	你是否已評估AI模型的潛在威脅（設計缺陷、技術缺陷、環境威脅）與風險（被惡意使用、濫用或不當使用的風險）及可能產生的後果？	
			
			■不適用	
	32	你是否告知使用者AI模型之（潛在）風險？	
			
			■不適用	
抵禦資安攻擊之韌性和安全性	
    33	你是否已就AI模型可能的弱點或漏洞，評估潛在的資安攻擊模式（例如：資料中毒 、模型迴避攻擊 、模型逆向攻擊 ）？	
			
			■不適用	
	34	AI模型在面對設計或技術故障、缺陷、中斷、攻擊、濫用、不當使用或惡意使用等風險或威脅時，是否可能產生不利、嚴重或破壞性之影響？	
			■否	
			
	35	針對AI模型是否有適當的資安管理程序或是否符合特定資安標準？	
			
			■不適用	
	36	你是否已採取必要措施避免AI模型受到潛在攻擊，並確保AI模型生命週期中的完整性、健全性和整體安全性（例如：紅隊測試、滲透測試或安全性更新）？	
			
			■不適用	
可靠性與再現性	
    37	你是否已依AI模型表現的穩定與可信賴狀況，評估AI決策之可靠性？ 	■是	說明：
			
			
	38	你是否已建立相關機制，以便AI模型發生改變時，能重新評估技術穩健性和安全性？ 	■是	說明：
			
			
	39	你是否已建立明確的流程，以監控AI模型是否達到預期目標？	■是	說明：
			
			
	40	你是否已測試並確保AI模型的再現性？	■是	說明：
			
			
	41	在AI模型可靠性和再現性較低的情況下，是否會造成危急、不利或破壞性的後果（例如：影響人身安全）？	
			■否	
			
	42	當AI模型得出的結果被評為低可靠性時，你是否已訂定適當程序加以矯正或改善？	■是	說明：
			
			
自主權與控制權 (Autonomy and Control)
AI 應用係以輔助人類決策，AI 科研人員對於AI 系統、軟體、演算法等技術開發，應致力讓人類能擁有完整且有效的自主權與控制權。
人為監督	
    43	你的AI模型是否有導入人為監督？	■無	說明：本研究之AI模型是根據大氣與土壤水觀測資料自動調整參數；在訓練或測試的過程中，並無涉於人類回饋且無此必要。
			
				
				
			
	44	針對AI模型對使用者所帶來的非預期負面影響，你是否已建立任何偵測和回報機制？	
			■否	
			
人類自治和自主性	
    45	AI模型是否可能有造成人的依賴性、引發成癮行為，或可能操控使用者行為的風險？	
			■否	
			
	46	你是否採取適當措施，以避免AI模型影響人類自主性或讓使用者過度依賴AI？	
			
			■不適用	
共榮共利 (Common Good and Well-being)
AI 科研人員應追求人類、社會、環境間的利益平衡與共同福祉，並致力於多元文化、社會包容、環境永續等，達成保障人類身心健康、創建全體人民利益、總體環境躍升之AI 社會。
社會包容與多元性	
    47	你是否已評估AI模型能對應到社會中各種偏好與能力（例如：AI模型使用者介面也能被特定需求、身心障礙或其他弱勢者使用）？	
			
			■不適用	
	48	AI模型是否影響人的工作與工作安排（例如：造成人力去技能化的風險(de-skilling risk)，或人員因此需提升新（數位）技能）？	
			
			■不適用	
社會福祉與民主	
    49	AI模型的使用，除了終端使用者之外，是否還會間接影響到其他利害關係人？	
			
			■不適用	
	50	AI模型的使用是否對社會整體或民主有其他潛在影響（包括正面與負面影響）？	
			
			■不適用	
	51	你是否採取行動或措施，以降低AI模型可能造成的潛在社會或民主傷害？	
			
			■不適用	
問責與溝通(Accountability and Communication)
基於社會公益與關係人利益之維護，AI 的發展與應用應致力於建立AI 系統、軟體、演算法等技術之問責與溝通機制，包括但不限於決策程序與結果的說明、使用者與受影響者之回饋管道等。
溝通	
    52	你是否已向使用者說明AI模型之技術限制和潛在風險（例如：準確程度和錯誤率）？	■是	說明：
			
			
	53	你是否已建立相關機制，以告知使用者AI模型所生決策之目的、判斷標準和侷限？	■是	說明：
			
			
	54	你是否向使用者說明AI模型曾使用哪些訓練資料？	■是	說明：
			
			
可稽核性	
    55	你是否建立促進AI模型可被稽核的機制（例如：開發過程之可追溯性；訓練資源之溯源；AI模型生命週期之相關紀錄；正面及負面影響之紀錄）？	■是	說明：
			
			
	56	你是否已盤點與AI模型相關之法律規範？	
			
			■不適用	
	57	你是否考慮建立AI倫理審查委員會(AI ethics review board)或類似機制，以探討整體可歸責性與倫理實務，包括潛在灰色地帶的討論？	
			
			■不適用	
	58	你是否建立程序或機制，以持續評估並監控AI模型是否遵循本評估表所列之倫理原則？	
			
			■不適用	
	59	你是否建立相關程序或機制，以處理不同倫理原則之間的衝突，並解釋最後權衡的決定？	
			
			■不適用	


